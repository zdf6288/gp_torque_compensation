#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Gaussian Process (Mixture-of-Experts) trajectory predictor.
- Translated all Chinese comments to English.
- Tidied formatting and removed duplicate helper definitions.
- Kept function signatures and behavior intact where possible.
"""

import traceback
from datetime import datetime  # kept in case of future logging/usage
import csv

import numpy as np
import torch
import matplotlib
import matplotlib.pyplot as plt

from icra_gp.skygp_online import SkyGP_MOE as SaoGP_MOE

# ==============================
# Global Configuration
# ==============================
SEED = 0
SAMPLE_HZ = 100                  # sampling frequency for equal-time resampling
TRAIN_RATIO = 1.0                # demo: use all data for training
MAX_EXPERTS = 80
MIN_POINTS_OFFLINE = 1
WINDOW_SIZE = None
METHOD_ID = 1                    # 1=polar->delta; 5=polar+delta->delta
DEFAULT_SPEED = 0.1             # speed used to convert polyline length to time for equal-time resampling
STOP_TAIL_ZERO_STEPS = 0         # set deltas to zero for the last N seeds near the end
np.random.seed(SEED)
torch.manual_seed(SEED)

# Tunable parameters
VERBOSE = False
ANCHOR_ANGLE = np.radians(30)  # angle (rad) for anchor correspondence
K_HIST = 10                      # seed length (history window)
NEAREST_K = 1
MAX_DATA_PER_EXPERT = 1000
BASE_SCALE = 100                  # initial guess for scale
ROLLOUT_STEPS = 500               # steps to rollout in demo
PHI0_K_PROBE = 500                        # segments to average for phi0 in anchor finding
PHI0_K_REF = 500                        # segments to average for phi0 in anchor finding
SELECT_HORIZON = 500
# ==============================
# Method Hyperparameters
# ==============================
METHOD_HPARAM = {
    1: {'adam_lr': 0.001, 'adam_steps': 200}
}

# ==============================
# GP Utilities
# ==============================
def torch_to_np(x: torch.Tensor) -> np.ndarray:
    """Detach a torch tensor and move to CPU as a NumPy array."""
    return x.detach().cpu().numpy()


class Standardizer:
    """Simple feature standardizer compatible with previous interface."""

    def fit(self, X: torch.Tensor, Y: torch.Tensor) -> "Standardizer":
        self.X_mean = X.mean(0)
        self.X_std = X.std(0).clamp_min(1e-8)
        self.Y_mean = Y.mean(0)
        self.Y_std = Y.std(0).clamp_min(1e-8)
        return self

    def x_transform(self, X: torch.Tensor) -> torch.Tensor:
        return (X - self.X_mean) / self.X_std

    def y_transform(self, Y: torch.Tensor) -> torch.Tensor:
        return (Y - self.Y_mean) / self.Y_std

    def y_inverse_transform(self, Yn: torch.Tensor) -> torch.Tensor:
        assert Yn.shape[-1] == self.Y_std.shape[0], (
            f"Dimension mismatch: Yn.shape={Yn.shape}, std={self.Y_std.shape}"
        )
        return Yn * self.Y_std + self.Y_mean

    # Backward compatible alias
    def y_inverse(self, Yn: torch.Tensor) -> torch.Tensor:
        return self.y_inverse_transform(Yn)


def rotate_to_fixed_frame(vectors: torch.Tensor, base_dir: torch.Tensor) -> torch.Tensor:
    """
    Rotate 2D vectors into a fixed frame defined by base_dir (x-axis).
    y-axis is perpendicular to base_dir.
    """
    base = base_dir / base_dir.norm()
    x_axis = base
    y_axis = torch.tensor([-base[1], base[0]], dtype=torch.float32)
    R = torch.stack([x_axis, y_axis], dim=1)
    return vectors @ R


def polar_feat_from_xy_torch(xy: torch.Tensor, origin: torch.Tensor) -> torch.Tensor:
    """Compute polar-like features [r, cos(theta), sin(theta)] relative to origin for each (x, y)."""
    xy = xy.float()
    origin = origin.to(xy)
    shifted = xy - origin
    r = torch.sqrt(shifted[..., 0] ** 2 + shifted[..., 1] ** 2)
    theta = torch.atan2(shifted[..., 1], shifted[..., 0])
    return torch.stack([r, torch.cos(theta), torch.sin(theta)], dim=-1)


def build_dataset(traj: torch.Tensor, k: int):
    """
    Build (X, Y) for GP:
      - X: polar features of the last k positions w.r.t. global origin (traj[0])
      - Y: next-step delta rotated into the averaged initial direction frame
    Optionally zero out the last STOP_TAIL_ZERO_STEPS labels.
    """
    deltas = traj[1:] - traj[:-1]
    T = traj.shape[0]
    Xs, Ys = [], []
    global_origin = traj[0]

    # Global base direction: average of first up-to-10 segment vectors from start
    end_idx = min(10, traj.shape[0] - 1)
    dirs = traj[1 : end_idx + 1] - traj[0]
    global_base_dir = dirs.mean(dim=0)

    # Number of tail steps to zero out
    stop_tail = int(globals().get("STOP_TAIL_ZERO_STEPS", 10))

    for t in range(k, T - 1):
        feats = []
        seed_pos = traj[t - k + 1 : t + 1]

        feats.append(polar_feat_from_xy_torch(seed_pos, global_origin).reshape(-1))
        Xs.append(torch.cat(feats))

        # Force label to zero near the end (in fixed base frame)
        if t >= (T - 1 - stop_tail):
            Ys.append(torch.zeros(2, dtype=torch.float32))
        else:
            y_delta = traj[t + 1] - traj[t]
            Ys.append(rotate_to_fixed_frame(y_delta.unsqueeze(0), global_base_dir)[0])

    print(
        f"Dataset built: input dim {Xs[0].shape[0]}, samples {len(Xs)} "
        f"(last {stop_tail} seeds have zero delta)."
    )
    return torch.stack(Xs), torch.stack(Ys)


def time_split(X: torch.Tensor, Y: torch.Tensor, train_ratio: float):
    """Temporal split for (X, Y)."""
    N = X.shape[0]
    ntr = int(N * train_ratio)
    return (X[:ntr], Y[:ntr]), (X[ntr:], Y[ntr:]), ntr


def train_moe(dataset, method_id: int = METHOD_ID):
    """
    Train a Mixture-of-Experts GP with streaming point additions.
    Returns the trained moe and the standardizer.
    """
    Xtr = dataset["X_train"]
    Ytr = dataset["Y_train"]
    Din = Xtr.shape[1]
    Dout = Ytr.shape[1]

    scaler = Standardizer().fit(Xtr, Ytr)
    Xn = torch_to_np(scaler.x_transform(Xtr))
    Yn = torch_to_np(scaler.y_transform(Ytr))

    moe = SaoGP_MOE(
        x_dim=Din,
        y_dim=Dout,
        max_data_per_expert=MAX_DATA_PER_EXPERT,
        nearest_k=NEAREST_K,
        max_experts=MAX_EXPERTS,
        replacement=False,
        min_points=10**9,
        batch_step=10**9,
        window_size=256,
        light_maxiter=60,
    )
    for i in range(Xn.shape[0]):
        moe.add_point(Xn[i], Yn[i])

    params = METHOD_HPARAM.get(method_id, {"adam_lr": 0.001, "adam_steps": 200})
    if hasattr(moe, "optimize_hyperparams") and params["adam_steps"] > 0:
        # æ–°ï¼ˆå…±äº«è¶…å‚ä¸€æ¬¡æ€§è®­ç»ƒï¼‰
        moe.optimize_hyperparams_global(
            max_iter=params["adam_steps"],
            verbose=False,
            window_size=WINDOW_SIZE,
            adam_lr=params["adam_lr"],
        )
    return {"moe": moe, "scaler": scaler, "input_dim": Din}


def moe_predict(info, feat_1xD: torch.Tensor):
    """Predict (mu, var) in original label space given a 1xD feature tensor."""
    moe, scaler = info["moe"], info["scaler"]
    x_n = torch_to_np(scaler.x_transform(feat_1xD.float()))  # (1, D)
    mu_n, var_n = moe.predict(x_n.squeeze(0))                # var_n æ˜¯æ ‡å‡†åŒ–ç©ºé—´
    mu_n = np.array(mu_n).reshape(1, -1)
    y = torch_to_np(scaler.y_inverse(torch.tensor(mu_n)))    # é¢„æµ‹å‡å€¼ -> åŸå§‹ç©ºé—´

    # æ–¹å·®ä¹Ÿæ˜ å°„å›åŸå§‹ç©ºé—´ï¼šVar[Y] = Var[Y_n] * (Y_std^2)
    Y_std = torch_to_np(scaler.Y_std).reshape(-1)            # (Dout,)
    var = np.array(var_n).reshape(-1) * (Y_std**2)           # (Dout,)
    return y, var


def rollout_reference(model_info, traj, start_t, h, k):
    assert start_t >= (k - 1), f"start_t={start_t} å¤ªå°ï¼Œè‡³å°‘éœ€è¦ {k - 1}"
    T = traj.shape[0]
    h = max(0, h)
    
    # âœ… ä¿æŒå’Œè®­ç»ƒæ—¶ä¸€è‡´ï¼šä½¿ç”¨ global origin å’Œ global base_dir
    global_origin = traj[0]
    if traj.shape[0] > 1:
        print("âœ… è®¡ç®—probeå…¨å±€æ–¹å‘ä¸ºå‰10æ®µå¹³å‡æ–¹å‘")
        end_idx = min(10, traj.shape[0]-1)
        dirs = traj[1:end_idx+1] - traj[0]
        global_base_dir = dirs.mean(dim=0)
    else:
        print("âš ï¸ è½¨è¿¹ç‚¹ä¸è¶³2ä¸ªï¼Œæ— æ³•è®¡ç®—å…¨å±€æ–¹å‘ï¼Œä½¿ç”¨é»˜è®¤æ–¹å‘")
        global_base_dir = torch.tensor([1.0, 0.0])

    # åˆå§‹åŒ–å†å²ä½ç½®å’Œ delta
    hist_pos = [traj[start_t - (k - 1) + i].clone() for i in range(k)]
    hist_del = []
    for i in range(k):
        idx = start_t - (k - 1) + i
        prev = traj[idx - 1] if idx - 1 >= 0 else traj[0]
        hist_del.append(traj[idx] - prev)

    cur_pos = hist_pos[-1].clone()
    preds_std = []  # å­˜å‚¨æ ‡å‡†åŒ–é¢„æµ‹
    preds_pos = []  # å­˜å‚¨å®é™…ä½ç½®ï¼ˆåæ ‡å‡†åŒ–åï¼‰
    vars_seq = []  # æ–°å¢ï¼šå­˜å‚¨æ¯æ­¥æ–¹å·®

    for _ in range(h):
        feats = []

        polar_feat = polar_feat_from_xy_torch(torch.stack(hist_pos[-k:]), global_origin)
        feats.append(polar_feat.reshape(1, -1))  # (1, 2K)

        x = torch.cat(feats, dim=1)  # shape (1, D)

        # GPé¢„æµ‹
        y_pred, var = moe_predict(model_info, x)  # ç°åœ¨æ‹¿åˆ° var
        y_pred = torch.tensor(y_pred, dtype=torch.float32)  # ç¡®ä¿ tensor ç±»å‹ä¸€è‡´
        # print(f"Predicted (std space): {y_pred.numpy()}")
        preds_std.append(y_pred[0])
        vars_seq.append(var)   # <--- ä¿å­˜æ–¹å·®

        gb = global_base_dir / global_base_dir.norm()
        R = torch.stack([gb, torch.tensor([-gb[1], gb[0]])], dim=1)
        step_world = y_pred @ R.T  # shape (1, 2)
        next_pos = cur_pos + step_world[0]
        next_del = step_world[0]
        
        # æ›´æ–°å†å²
        hist_pos.append(next_pos)
        hist_del.append(next_del)
        cur_pos = next_pos
        preds_pos.append(next_pos)
        
    preds = torch.stack(preds_pos, dim=0)

    # Ground truth (å¯é€‰ï¼Œä»…è°ƒè¯•ç”¨)
    gt = traj[start_t + 1: start_t + 1 + h]
    return preds, gt, h, np.array(vars_seq)


# ==============================
# Resampling & Geometry Utilities
# ==============================
def resample_polyline_equal_dt(points_xy, sample_hz: float, speed: float) -> np.ndarray:
    """
    Equal-time resample a polyline given a nominal speed.
    """
    pts = np.asarray(points_xy, dtype=np.float32)
    if pts.shape[0] < 2:
        return pts
    seg = pts[1:] - pts[:-1]
    seg_len = np.linalg.norm(seg, axis=1)
    L = float(np.sum(seg_len))
    if L <= 1e-8:
        return pts[:1]
    T_total = L / float(speed)
    dt = 1.0 / float(sample_hz)
    t_samples = np.arange(0.0, T_total + 1e-9, dt)
    s_samples = (t_samples / T_total) * L
    cum_s = np.concatenate([[0.0], np.cumsum(seg_len)])
    out = []
    j = 0
    for s in s_samples:
        while j < len(seg_len) - 1 and s > cum_s[j + 1]:
            j += 1
        ds = s - cum_s[j]
        r = 0.0 if seg_len[j] < 1e-9 else ds / seg_len[j]
        p = pts[j] + r * seg[j]
        out.append(p)
    return np.asarray(out, dtype=np.float32)


def resample_to_k(points_xy, k: int) -> np.ndarray:
    """
    Resample a polyline to exactly k points by arc-length parameterization.
    """
    pts = np.asarray(points_xy, dtype=np.float64)
    if pts.shape[0] < 2:
        return (
            np.repeat(pts[:1], k, axis=0)
            if pts.size
            else np.zeros((k, 2), dtype=np.float64)
        )
    seg = pts[1:] - pts[:-1]
    seg_len = np.linalg.norm(seg, axis=1)
    cum = np.concatenate([[0.0], np.cumsum(seg_len)])
    L = cum[-1]
    if L < 1e-9:
        return np.tile(pts[:1], (k, 1))
    s = np.linspace(0.0, L, k)
    out = []
    j = 0
    for si in s:
        while j < len(seg_len) - 1 and si > cum[j + 1]:
            j += 1
        ds = si - cum[j]
        r = 0.0 if seg_len[j] < 1e-9 else ds / seg_len[j]
        p = pts[j] + r * seg[j]
        out.append(p)
    return np.asarray(out, dtype=np.float64)


# ==============================
# Angle Helpers (relative to start tangent)
# ==============================
def _wrap_pi(a: np.ndarray) -> np.ndarray:
    """Wrap angle to (-pi, pi]."""
    return ((a + np.pi) % (2 * np.pi)) - np.pi


def estimate_start_tangent(xy, k: int = 5) -> float:
    """
    Initial heading Ï†0 by chord-average from the origin:
    mean( P[i] - P[0] ), i=1..k (clamped by available points).
    This matches the 'global_base_dir' used in training/rollout.
    Returns radians in (-pi, pi].
    """
    P = np.asarray(xy, dtype=np.float64)
    if P.shape[0] < 2:
        return 0.0
    # use up to k chords from the origin
    m = int(min(max(1, P.shape[0] - 1), max(1, k)))
    dirs = P[1:m+1] - P[0]
    v = dirs.mean(axis=0)
    if not np.isfinite(v).all() or np.linalg.norm(v) < 1e-12:
        v = P[1] - P[0]
    return float(np.arctan2(v[1], v[0]))


def angles_relative_to_start_tangent(points, k_hist: int, min_r: float = 1e-3):
    """
    Compute relative angles w.r.t. the initial tangent and a validity mask (r > min_r).
    """
    P = np.asarray(points, dtype=np.float64)
    if len(P) == 0:
        return np.array([]), np.zeros(0, dtype=bool)
    o = P[0]
    phi0 = estimate_start_tangent(P, k=k_hist)
    
    v = P - o
    r = np.linalg.norm(v, axis=1)
    th = np.arctan2(v[:, 1], v[:, 0])
    th_rel = _wrap_pi(th - phi0)
    mask = r > min_r
    return th_rel, mask


def _angles_with_phi0(points, k_hist: int, min_r: float):
    """
    Compatibility wrapper.
    - If your angles_relative_to_start_tangent returns (angles, mask, phi0), return as-is.
    - Else compute phi0 here and return (angles, mask, phi0).
    """
    out = angles_relative_to_start_tangent(points, k_hist=k_hist, min_r=min_r)
    if isinstance(out, tuple) and len(out) == 3:
        print("âš ï¸ Warning: angles_relative_to_start_tangent already returns phi0; redundant computation.")
        return out  # (angles, mask, phi0)
    elif isinstance(out, tuple) and len(out) == 2:
        angles, mask = out
        phi0 = estimate_start_tangent(points, k=k_hist)
        return angles, mask, phi0
    else:
        raise RuntimeError("Unexpected return format from angles_relative_to_start_tangent")


def build_relative_angles(xy, origin_idx: int = 0, min_r: float = 1e-6) -> np.ndarray:
    """
    Build a relative angle array aligned to origin_idx; preceding entries are NaN.
    """
    P = np.asarray(xy, dtype=np.float64)
    N = len(P)
    if N == 0:
        return np.array([], dtype=np.float64)
    sub = P[origin_idx:]
    th_rel_sub, _ = angles_relative_to_start_tangent(sub, k_hist=K_HIST, min_r=min_r)
    out = np.full(N, np.nan, dtype=np.float64)
    out[origin_idx : origin_idx + len(th_rel_sub)] = th_rel_sub
    return out


def angle_diff(a: float, b: float) -> float:
    """Compute signed difference between angles a and b (wrapped to (-pi, pi])."""
    return _wrap_pi(a - b)


def angle_diff_mod_pi(a: float, b: float) -> float:
    """Compute minimal signed difference between a and b in (-pi, pi]."""
    return ((a - b + np.pi) % (2 * np.pi)) - np.pi


def first_index_reach_threshold(
    angles,
    mask,
    target,
    *,
    inclusive: bool = True
) -> int:
    """
    å¯¹ç›¸é‚»æœ‰æ•ˆç‚¹ (i_prev -> i_curr) åšåŒºé—´åˆ¤å®šï¼šå¦‚æœ target æˆ– -target
    ä»»æ„ä¸€ä¸ªåœ¨ä» angles[i_prev] åˆ° angles[i_curr] çš„æœ€å°æœ‰ç¬¦å·æ—‹è½¬åŒºé—´å†…ï¼Œ
    å°±è¿”å› i_prevã€‚å¦åˆ™å›é€€åˆ°äºŒè€…ä¸­â€œæ›´æ¥è¿‘â€çš„ç‚¹çš„å‰ä¸€ä¸ªç´¢å¼•ã€‚

    - è§’åº¦å•ä½ï¼šå¼§åº¦
    - ä½¿ç”¨ (-pi, pi] çš„æœ€å°æœ‰ç¬¦å·å·®å¤„ç†ç¯ç»•
    """
    idxs = np.where(mask)[0]
    if idxs.size == 0:
        return 0
    if idxs.size == 1:
        return int(idxs[0])

    def _diff(a, b):
        # æœ€å°æœ‰ç¬¦å·å·® in (-pi, pi]
        return angle_diff_mod_pi(a, b)

    def _bracket_hit(a, b, t) -> bool:
        d  = _diff(b, a)   # a -> b çš„æœ€å°æ—‹è½¬
        dt = _diff(t, a)   # a -> t çš„æœ€å°æ—‹è½¬
        if d > 0:
            return (0.0 <= dt <= d) if inclusive else (0.0 < dt < d)
        elif d < 0:
            return (d <= dt <= 0.0) if inclusive else (d < dt < 0.0)
        else:
            # d == 0ï¼ša ä¸ b é‡åˆï¼Œä»…å½“ t == a æ—¶å‘½ä¸­ï¼ˆè€ƒè™‘æ•°å€¼å®¹å·®ï¼‰
            return inclusive and (abs(dt) <= 1e-12)

    ang = np.asarray(angles, dtype=float)
    t_pos = float(target)
    t_neg = -t_pos

    # æ‰«æç›¸é‚»æœ‰æ•ˆç´¢å¼•å¯¹
    for j in range(1, len(idxs)):
        i_prev = idxs[j - 1]
        i_curr = idxs[j]
        a = float(ang[i_prev])
        b = float(ang[i_curr])

        if _bracket_hit(a, b, t_pos) or _bracket_hit(a, b, t_neg):
            return int(i_prev)

    # å›é€€ï¼šé€‰æ‹©å¯¹ {+target, -target} ä¸­æ›´æ¥è¿‘çš„é‚£ä¸ª
    diffs_pos = np.array([abs(_diff(float(ang[i]), t_pos)) for i in idxs])
    diffs_neg = np.array([abs(_diff(float(ang[i]), t_neg)) for i in idxs])
    diffs = np.minimum(diffs_pos, diffs_neg)

    k = int(idxs[int(np.argmin(diffs))])
    pos = int(np.where(idxs == k)[0][0])
    return int(idxs[pos - 1]) if pos > 0 else k


def last_window_rel_angles(points, W: int, min_r: float = 1e-3):
    """
    Return the last window of relative angles and mask using window size W.
    """
    P = np.asarray(points, dtype=np.float64)
    if P.shape[0] < 2:
        return None, None
    W = int(max(2, min(W if W is not None else 10, P.shape[0])))
    th, m = angles_relative_to_start_tangent(P, k_hist=W, min_r=min_r)
    end = len(th) - 1
    start = max(0, end - (W - 1))
    return th[start : end + 1], m[start : end + 1]


# ==============================
# Anchor Visualization & Correspondence
# ==============================
def _compute_base_unit_vec(points, n_segments: int = 10) -> np.ndarray:
    """
    Compute a base unit vector as the mean of the first n_segments tangents.
    """
    pts = np.asarray(points, dtype=np.float64)
    m = min(n_segments, pts.shape[0] - 1)
    if m < 1:
        return np.array([1.0, 0.0], dtype=np.float64)
    seg = np.diff(pts[: m + 1], axis=0)  # first m segments
    n = np.linalg.norm(seg, axis=1, keepdims=True)
    n[n < 1e-12] = 1.0
    u = seg / n  # unit tangents
    v = u.mean(axis=0)  # average direction
    if np.linalg.norm(v) < 1e-12:
        v = seg[0]
    return v / max(np.linalg.norm(v), 1e-12)


def get_anchor_correspondence(ref_pts, probe_pts, angle_target, *, min_r: float = 1e-3, n_segments_base: int = 10):
    ref_pts = np.asarray(ref_pts, dtype=np.float64)
    probe_pts = np.asarray(probe_pts, dtype=np.float64)
    assert ref_pts.shape[0] >= 2 and probe_pts.shape[0] >= 2, "ref/probe require at least 2 points"

    # ç”¨â€œåŸå§‹è½¨è¿¹ä¸Šçš„å¼¦å‘é‡å¹³å‡â€å®šä¹‰çš„ Ï†0
    ref_ang, ref_mask, _ = _angles_with_phi0(ref_pts,  k_hist=PHI0_K_REF, min_r=min_r)
    pro_ang, pro_mask, _ = _angles_with_phi0(probe_pts, k_hist=PHI0_K_PROBE, min_r=min_r)

    i_ref = first_index_reach_threshold(ref_ang, ref_mask, angle_target, inclusive=True)
    i_pro = first_index_reach_threshold(pro_ang, pro_mask, angle_target, inclusive=True)
    print(
        f"ğŸ¯ Target angle {angle_target:.2f} rad | "
        f"ref idx={i_ref}, angle={ref_ang[i_ref]:.2f} | "
        f"probe idx={i_pro}, angle={pro_ang[i_pro]:.2f}"
    )

    # Vectors from origin to anchor points
    o_ref, p_ref = ref_pts[0], ref_pts[i_ref]
    v_ref = p_ref - o_ref
    o_pro, p_pro = probe_pts[0], probe_pts[i_pro]
    v_pro = p_pro - o_pro

    # Base unit vectors (mean of first n_segments directions)
    u_ref = _compute_base_unit_vec(ref_pts, n_segments=n_segments_base)
    u_pro = _compute_base_unit_vec(probe_pts, n_segments=n_segments_base)

    return {
        "ref_index": i_ref,
        "ref_vector": v_ref,
        "ref_point": p_ref,
        "ref_base_unit": u_ref,
        "probe_index": i_pro,
        "probe_vector": v_pro,
        "probe_point": p_pro,
        "probe_base_unit": u_pro,
    }


def plot_anchor_vectors_from_gp(gp):
    """
    Plot, in a single figure:
      - Reference (resampled) trajectory
      - Probe (resampled) trajectory
      - Two anchor vectors: origin â†’ anchor (for ref and for probe)
    Depends on gp.seed_end and gp.probe_end being set in predict_from_probe().
    """
    if gp.sampled is None or not gp.probe_pts or gp.seed_end is None or gp.probe_end is None:
        print("â—No anchors to plot (run train_gp / predict_from_probe first).")
        return

    ref = gp.sampled.detach().cpu().numpy()  # resampled reference trajectory
    probe = np.asarray(gp.probe_pts, dtype=np.float64)  # resampled probe trajectory
    i_ref = int(gp.seed_end)
    i_pro = int(gp.probe_end)

    # Bounds check
    if not (0 <= i_ref < len(ref)) or not (0 <= i_pro < len(probe)):
        print("â—Anchor indices out of range; cannot plot.")
        return

    o_ref, p_ref = ref[0], ref[i_ref]
    o_pro, p_pro = probe[0], probe[i_pro]

    fig, ax = plt.subplots(1, 1, figsize=(8, 6))
    ax.plot(ref[:, 0], ref[:, 1], "-", label="Reference (resampled)")
    ax.plot(probe[:, 0], probe[:, 1], "-", label="Probe (resampled)")

    # Two originâ†’anchor vectors
    ax.plot([o_ref[0], p_ref[0]], [o_ref[1], p_ref[1]], "--", linewidth=2, label=f"Ref anchor vec (idx={i_ref})")
    ax.plot([o_pro[0], p_pro[0]], [o_pro[1], p_pro[1]], "--", linewidth=2, label=f"Probe anchor vec (idx={i_pro})")

    # Markers
    ax.scatter([o_ref[0], p_ref[0]], [o_ref[1], p_ref[1]], s=40, marker="o")
    ax.scatter([o_pro[0], p_pro[0]], [o_pro[1], p_pro[1]], s=40, marker="x")

    ax.set_aspect("equal", adjustable="box")
    ax.grid(True, alpha=0.3)
    ax.legend()
    ax.set_title("Anchor Corresponding Vectors (origin â†’ anchor)")
    plt.tight_layout()
    plt.savefig("anchor_vectors_main.png", dpi=150, bbox_inches="tight")


def _phi0_unit(points, k_hist=K_HIST):
    """
    Compute the unit vector of the initial tangent (phi0) at the origin.
    Returns (unit_vector, phi0_radians).
    """
    pts = np.asarray(points, dtype=np.float64)
    phi0 = estimate_start_tangent(pts, k=k_hist)
    u = np.array([np.cos(phi0), np.sin(phi0)], dtype=np.float64)
    return u, phi0


def plot_relative_angle_bases_from_gp(gp, *, k_hist=K_HIST, vec_len=None, filename="relative_angle_bases.png"):
    """
    Plot the two base vectors used by the relative-angle computation (phi0 of ref & probe).
    Uses the current resampled reference (gp.sampled) and probe (gp.probe_pts).
    """
    if gp.sampled is None or not gp.probe_pts:
        print("â—No data to plot base vectors (run train_gp and predict_from_probe first).")
        return

    # Resampled reference & probe used in your pipeline
    ref = gp.sampled.detach().cpu().numpy()
    probe = np.asarray(gp.probe_pts, dtype=np.float64)

    # Compute unit base vectors (phi0) at each origin
    ref_raw_for_plot = gp.best_ref.get("raw", gp.ref_pts_raw) if hasattr(gp, "best_ref") and gp.best_ref else gp.ref_pts_raw
    u_ref, phi_ref = _phi0_unit(ref_raw_for_plot, k_hist=PHI0_K_REF)
    u_pro, phi_pro = _phi0_unit(gp.probe_pts_raw, k_hist=PHI0_K_PROBE)

    # Choose a nice arrow length if not provided
    if vec_len is None:
        all_pts = np.vstack([ref, probe])
        span = float(np.max(np.ptp(all_pts, axis=0)))
        vec_len = max(1e-9, 0.15 * span)

    o_ref = ref[0]
    o_pro = probe[0]

    fig, ax = plt.subplots(1, 1, figsize=(8, 6))
    ax.plot(ref[:, 0], ref[:, 1], "-", label="Reference (resampled)")
    ax.plot(probe[:, 0], probe[:, 1], "-", label="Probe (resampled)")

    # Draw phi0 vectors from each origin
    ax.quiver(o_ref[0], o_ref[1], vec_len * u_ref[0], vec_len * u_ref[1],
              angles="xy", scale_units="xy", scale=1, width=0.004,
              label=f"Ref Ï†0 (k={k_hist})")
    ax.quiver(o_pro[0], o_pro[1], vec_len * u_pro[0], vec_len * u_pro[1],
              angles="xy", scale_units="xy", scale=1, width=0.004,
              label=f"Probe Ï†0 (k={k_hist})")

    ax.set_aspect("equal", adjustable="box")
    ax.grid(True, alpha=0.3)
    ax.legend()
    ax.set_title("Base vectors for relative-angle computation (Ï†0 at origin)")
    plt.tight_layout()
    plt.savefig(filename, dpi=150, bbox_inches="tight")
    print(f"Saved base vectors plot to {filename} | "
          f"ref Ï†0={np.degrees(phi_ref):.2f}Â°, probe Ï†0={np.degrees(phi_pro):.2f}Â°")

def _closest_index(pt, arr):
    arr = np.asarray(arr, dtype=np.float64)
    d = np.linalg.norm(arr - pt[None, :], axis=1)
    return int(np.argmin(d))

# ==============================
# Predictor Class
# ==============================
class GP_predictor:
    def __init__(self):
        # State
        self.ref_pts = []
        self.sampled = None
        
        self.ref_pts_raw = None      # åŸå§‹ ref
        self.probe_pts_raw = None    # åŸå§‹ probe
        
        self.model_info = None
        self.seed_end = None

        # Probe end index
        self.probe_end = None
        self.dtheta_manual = 0.0
        self.scale_manual = 1.0

        self.probe_pts = []

        # Termination (in probe frame)
        self.probe_goal = None         # predicted finish point in probe coordinates
        self.goal_stop_eps = 0.005      # Euclidean distance threshold for stopping

        # Multiple reference trajectories
        self.refs = []
        
        self.anchor = None

    def predict_on_transformed_probe(self):
        if not hasattr(self, "best_ref") or self.best_ref is None:
            print("â— æœªæ‰¾åˆ°æœ€ä½³å‚è€ƒè½¨è¿¹ (è¯·å…ˆç”» probe)")
            return
        if len(self.probe_pts) < K_HIST:
            print("â— probe å¤ªçŸ­")
            return

        # === Step 0: æ•°æ®å‡†å¤‡ ===
        ref_np = self.best_ref['sampled'].numpy()
        model_info = self.best_ref['model_info']
        probe_np = np.asarray(self.probe_pts, dtype=np.float64)

        # === Step 1: Î”Î¸ å’Œ scale ===
        dtheta = self.dtheta_manual
        spatial_scale = self.scale_manual
        print(f"ğŸ“ æ‰‹åŠ¨è®¾å®š: Î”Î¸={np.degrees(dtheta):.2f}Â°, scale={spatial_scale:.3f}")

        # === Step 2: probe â†’ ref frame ===
        c, s = np.cos(-dtheta), np.sin(-dtheta)
        R_inv = np.array([[c, -s], [s, c]])
        probe_origin = probe_np[0]
        probe_in_ref = ((probe_np - probe_origin) @ R_inv.T) / spatial_scale

        # === Step 3: ç›®æ ‡ç»ˆç‚¹ = refæœ€åä¸€ç‚¹æ˜ å°„å› probe ===
        c_f, s_f = np.cos(dtheta), np.sin(dtheta)
        R_fwd = np.array([[c_f, -s_f], [s_f, c_f]])
        ref_vec_total = ref_np[-1] - ref_np[0]
        probe_goal = probe_origin + spatial_scale * (R_fwd @ ref_vec_total)
        self.probe_goal = probe_goal

        # === Step 4: GP rollout in ref frame ===
        start_t = probe_in_ref.shape[0] - 1
        h = 600

        preds_ref, gt_ref, h_used, vars_ref = rollout_reference(
            model_info,
            torch.tensor(probe_in_ref, dtype=torch.float32),
            start_t=start_t,
            h=h,
            k=K_HIST
        )

        preds_ref_np = preds_ref.numpy()
        vars_ref = np.array(vars_ref)

        # === Step 5: æˆªæ–­é€»è¾‘ ===
        preds_world = (preds_ref_np * spatial_scale) @ R_fwd.T + probe_origin
        if self.probe_goal is not None and preds_world.shape[0] > 0:
            dists = np.linalg.norm(preds_world - self.probe_goal[None, :], axis=1)
            hits = np.where(dists <= self.goal_stop_eps)[0]
            cut_idx_final = None
            for cut_idx in hits:
                var_at_hit = np.max(vars_ref[cut_idx])
                print(f"[Probe-based] idx={cut_idx}, d={dists[cut_idx]:.3f}, var={var_at_hit:.6f}")
                # if var_at_hit > 0.0001:   # âœ… åŒæ—¶æ»¡è¶³æ¡ä»¶
                cut_idx_final = cut_idx
                break
            if cut_idx_final is not None:
                print(f"âœ‚ï¸ Probe-based æˆªæ–­åˆ° {cut_idx_final} ç‚¹")
                preds_ref_np = preds_ref_np[:cut_idx_final]
                vars_ref = vars_ref[:cut_idx_final]

        # === Step 6: æ–¹å·®å¸¦ï¼ˆref frame â†’ probe frameï¼‰===
        stds_ref = np.sqrt(vars_ref)
        upper_ref = preds_ref_np + 2 * stds_ref
        lower_ref = preds_ref_np - 2 * stds_ref
        polygon_ref = np.vstack([upper_ref, lower_ref[::-1]])

        preds_world = (preds_ref_np * spatial_scale) @ R_fwd.T + probe_origin
        polygon_world = (polygon_ref * spatial_scale) @ R_fwd.T + probe_origin
        self.pred_vars = vars_ref

        # if hasattr(self, "poly_probe") and self.poly_probe:
        #     self.poly_probe.remove()
        # self.poly_probe = self.ax.fill(
        #     polygon_world[:, 0], polygon_world[:, 1],
        #     color='green', alpha=0.2, label='Probe-based Â±2Ïƒ region'
        # )[0]

        # self.update_scaled_pred(preds_world)
        
        # åŸå§‹è½¨è¿¹ä¸Šçš„é¢„æµ‹ç‚¹
        self.preds_ref = preds_ref_np
        print(f"âœ… Probe-based é¢„æµ‹å®Œæˆ | Î”Î¸={np.degrees(dtheta):.1f}Â°, scale={spatial_scale:.3f}")
        return preds_world

    # --- Public API (batch-style) ---
    def train_gp(self, ref_traj):
        ref_traj = np.asarray(ref_traj, dtype=np.float32)
        if ref_traj.ndim != 2 or ref_traj.shape[1] != 2:
            raise ValueError("ref_traj must be shaped (N, 2)")
        self.ref_pts_raw = ref_traj.tolist()     # <<< ä¿å­˜åŸå§‹
        self.ref_pts = self.ref_pts_raw[:]       # ä¿æŒåŒå€¼ï¼ˆhandle_train å†…ä¼šé‡é‡‡æ ·ä½†ä¸è¦†ç›– rawï¼‰
        self.handle_train()
        if hasattr(self, "refs") and self.refs:
            self.best_ref = self.refs[-1]
        return self.model_info

    def predict_from_probe(self, probe_traj):
        if self.model_info is None and not self.refs:
            raise RuntimeError("Call train_gp(ref_traj) before prediction.")

        probe_traj = np.asarray(probe_traj, dtype=np.float64)
        self.probe_pts_raw = probe_traj.tolist()

        if probe_traj.ndim != 2 or probe_traj.shape[1] != 2:
            raise ValueError("probe_traj must be shaped (M, 2)")
        if probe_traj.shape[0] < 2:
            raise ValueError("probe_traj requires at least two points")

        # ç­‰æ—¶é‡é‡‡æ · probeï¼ˆç”¨äº GP ç‰¹å¾æ„é€ /rolloutï¼‰
        probe_eq = resample_polyline_equal_dt(probe_traj.astype(np.float32), SAMPLE_HZ, DEFAULT_SPEED)
        if probe_eq.shape[0] >= 2:
            self.probe_pts = probe_eq.tolist()
            print(f"ğŸ”„ Probe resampled to {len(self.probe_pts)} points.")
        probe_eq_np  = np.asarray(self.probe_pts, dtype=np.float64)
        probe_raw_np = np.asarray(self.probe_pts_raw, dtype=np.float64)

        # === å…³é”®ï¼šåœ¨å¤šå‚è€ƒä¸­é€‰å¹³å‡é¢„æµ‹æ–¹å·®æœ€ä½è€…ï¼ˆç”¨10æ­¥ï¼‰ ===
        if len(self.refs) == 0:
            raise RuntimeError("No available reference; please train first.")

        # === ç”¨ MSE é€‰æœ€åŒ¹é…çš„å‚è€ƒï¼ˆé»˜è®¤å¯¹é½åˆ°é”šç‚¹ï¼Œæ¯”è¾ƒå‰ 100 ç‚¹ï¼‰ ===
        best_idx, best_pack, best_mse = self._choose_best_ref_by_mse(
            probe_eq_np, probe_raw_np, horizon=100, align_on_anchor=False
        )
        if best_idx is None:
            print("â— Failed to choose a best reference (insufficient data).")
            return None

        out, dtheta, scale = best_pack
        self.best_ref = self.refs[best_idx]
        print(f"âœ… Selected reference #{best_idx} for prediction.")
        self.sampled  = self.best_ref["sampled"]          # è®©å›¾é‡Œ/åç»­å‡½æ•°ä½¿ç”¨â€œæœ€ä½³å‚è€ƒâ€çš„é‡‡æ ·è½¨è¿¹
        self.anchor = out
        self.dtheta_manual = dtheta
        self.scale_manual  = scale
        # å°†åŸå§‹ anchor ç‚¹æ˜ å°„åˆ°â€œé‡é‡‡æ ·åçš„â€ç´¢å¼•ï¼Œä¾› plot_anchor_vectors_from_gp ä½¿ç”¨
        ref_resampled = self.sampled.detach().cpu().numpy()           # (Nr,2)
        probe_resampled = np.asarray(self.probe_pts, dtype=np.float64) # (Np,2)

        self.seed_end  = _closest_index(self.anchor["ref_point"],   ref_resampled)
        self.probe_end = _closest_index(self.anchor["probe_point"], probe_resampled)

        # æœ€ç»ˆç”¨â€œæœ€ä½³å‚è€ƒâ€çš„ Î”Î¸/scale åšå®Œæ•´ rollout
        preds_world = self.predict_on_transformed_probe()
        return preds_world

    # --- Internal training routine (reference frame) ---
    def handle_train(self):
        if len(self.ref_pts) < 2:
            print("â— Draw the reference trajectory first (at least 2 points).")
            return

        sampled = resample_polyline_equal_dt(self.ref_pts, SAMPLE_HZ, DEFAULT_SPEED)
        if sampled.shape[0] < K_HIST + 2:
            print(f"â— Too few samples {sampled.shape[0]} < {K_HIST + 2}")
            return

        NOISE_STD = 0.000
        sampled_noisy = sampled + np.random.normal(0, NOISE_STD, size=sampled.shape)

        self.sampled = torch.tensor(sampled_noisy, dtype=torch.float32)

        X, Y = build_dataset(self.sampled, K_HIST)
        (Xtr, Ytr), (Xte, Yte), ntr = time_split(X, Y, TRAIN_RATIO)
        ds = {"X_train": Xtr, "Y_train": Ytr, "X_test": Xte, "Y_test": Yte, "n_train": ntr}
        self.model_info = train_moe(ds, METHOD_ID)

        # Record as one reference model
        self.refs.append(dict(
            sampled=self.sampled,
            model_info=self.model_info,
            raw=np.array(self.ref_pts_raw, dtype=np.float32)  # ä¿å­˜è¯¥å‚è€ƒçš„åŸå§‹è½¨è¿¹
        ))

    def _choose_best_ref_by_mse(
        self,
        probe_eq_np: np.ndarray,
        probe_raw_np: np.ndarray,
        *,
        horizon: int | None = 100,     # æ¯”å¦‚åªçœ‹å‰ 100 ä¸ªå¯¹é½åçš„ç‚¹ï¼›None è¡¨ç¤ºç”¨å…¨éƒ¨é‡å æ®µ
        align_on_anchor: bool = True   # æ˜¯å¦ç”¨é”šç‚¹å¯¹é½ä¸¤ä¸ªåºåˆ—çš„ç´¢å¼•ï¼ˆæ›´ç¨³ï¼‰
    ):
        """
        å¯¹æ¯æ¡å‚è€ƒï¼š
        1) ç”¨åŸå§‹ ref/probe è®¡ç®—é”šç‚¹ -> å¾—åˆ° Î”Î¸ ä¸ scale
        2) å°†â€œå‚è€ƒçš„ç­‰æ—¶é‡é‡‡æ ·è½¨è¿¹â€æ—‹è½¬/ç¼©æ”¾åˆ° probe åæ ‡ç³»
        3) æŒ‰ç´¢å¼•ä¸€ä¸€å¯¹åº”è®¡ç®—å¹³æ–¹è·ç¦»å¹¶å–å‡å€¼ï¼ˆMSEï¼‰
            - è‹¥ align_on_anchor=Trueï¼šè®© ref çš„é”šç‚¹ç´¢å¼•ä¸ probe çš„é”šç‚¹ç´¢å¼•å¯¹é½åå†æ¯”
            - horizon é™åˆ¶åªæ¯”è¾ƒå‰ horizon ä¸ªé‡å ç‚¹
        è¿”å›: (best_idx, (anchor_out, dtheta, scale), best_mse)
        """
        best_idx, best_mse, best_pack = None, float("inf"), None

        for ridx, ref in enumerate(self.refs):
            ref_raw = ref.get("raw", None)
            if ref_raw is None or ref["model_info"] is None:
                continue

            # 1) é”šç‚¹/å°ºåº¦ï¼ˆç”¨åŸå§‹è½¨è¿¹åšè§’åº¦åŸºä¸é”šç‚¹ï¼‰
            out = get_anchor_correspondence(
                ref_raw, probe_raw_np, angle_target=ANCHOR_ANGLE, n_segments_base=10
            )
            v_ref, v_pro = out["ref_vector"], out["probe_vector"]
            dtheta = float(np.arctan2(v_pro[1], v_pro[0]) - np.arctan2(v_ref[1], v_ref[0]))
            scale  = float(np.linalg.norm(v_pro) / max(np.linalg.norm(v_ref), 1e-6))

            # 2) å‚è€ƒï¼ˆç­‰æ—¶é‡é‡‡æ ·ï¼‰â†’ probe åæ ‡ç³»
            ref_samp = ref["sampled"].detach().cpu().numpy()     # (Nr,2)
            c, s = np.cos(dtheta), np.sin(dtheta)
            R = np.array([[c, -s], [s,  c]], dtype=np.float64)
            ref_in_probe = (ref_samp - ref_samp[0]) @ R.T * scale + probe_eq_np[0]

            # 3) é€‰æ‹©å¯¹é½çš„é‡å æ®µå¹¶è®¡ç®— MSE
            if align_on_anchor:
                # ç”¨é”šç‚¹åœ¨â€œé‡é‡‡æ ·åºåˆ—â€ä¸­çš„æœ€è¿‘ç´¢å¼•åšå¯¹é½
                i_ref_res = _closest_index(out["ref_point"],   ref_samp)
                i_pro_res = _closest_index(out["probe_point"], probe_eq_np)
                offset = int(i_pro_res - i_ref_res)  # ref åºåˆ—éœ€è¦å‘å³ç§»å¤šå°‘æ‰èƒ½å¯¹é½ probe

                start_ref = max(0, -offset)
                start_pro = max(0,  offset)
                n_overlap = min(ref_in_probe.shape[0] - start_ref,
                                probe_eq_np.shape[0] - start_pro)
                if n_overlap <= 0:
                    continue
                if horizon is not None:
                    n_overlap = min(n_overlap, int(horizon))

                A = ref_in_probe[start_ref : start_ref + n_overlap]
                B = probe_eq_np[start_pro : start_pro + n_overlap]
            else:
                n_overlap = min(ref_in_probe.shape[0], probe_eq_np.shape[0])
                if n_overlap <= 0:
                    continue
                if horizon is not None:
                    n_overlap = min(n_overlap, int(horizon))
                A = ref_in_probe[:n_overlap]
                B = probe_eq_np[:n_overlap]

            mse = float(np.mean(np.sum((A - B) ** 2, axis=1)))
            print(f"Ref #{ridx}: MSE@{n_overlap} = {mse:.6f}")

            if mse < best_mse:
                best_mse  = mse
                best_idx  = ridx
                best_pack = (out, dtheta, scale)

        return best_idx, best_pack, best_mse

# ==============================
# Main (batch run)
# ==============================
if __name__ == "__main__":
    import time, glob

    # 1) å¤šæ¡è®­ç»ƒè½¨è¿¹
    train_files = sorted(glob.glob("hand_train_*.csv"))  # æ¯”å¦‚ hand_train_1.csv, hand_train_2.csv, ...
    gp = GP_predictor()
    for fp in train_files:
        rows = []
        with open(fp, "r") as f:
            r = csv.DictReader(f)
            for row in r:
                rows.append(row)
        ref_xy = np.array([[float(r["x_actual"]), float(r["y_actual"])] for r in rows], dtype=np.float32)
        print(f"Training on {fp}, shape={ref_xy.shape}")
        gp.train_gp(ref_xy)

    # 2) åŠ è½½ä¸€æ¡ probe
    rows = []
    with open("hand_probe_c.csv", "r") as f:
        r = csv.DictReader(f)
        for row in r:
            rows.append(row)
    probe_xy = np.array([[float(r["x_actual"]), float(r["y_actual"])] for r in rows], dtype=np.float32)

    # 3) é¢„æµ‹ï¼ˆå†…éƒ¨ä¼šå…ˆç”¨æ¯ä¸ªå‚è€ƒ GP é¢„æµ‹10æ­¥é€‰æœ€ä¼˜ï¼Œå†å®Œæ•´ rolloutï¼‰
    preds = gp.predict_from_probe(probe_xy)

    # >>> add transformed reference overlay <<<
    # ä» best_ref å–å‚è€ƒæ›²çº¿ï¼ˆä¼˜å…ˆåŸå§‹ rawï¼Œè‹¥æ²¡æœ‰å°±ç”¨ sampledï¼‰
    ref_sel = gp.best_ref.get("raw")
    if ref_sel is None:
        ref_sel = gp.best_ref["sampled"].detach().cpu().numpy()

    # ç”¨æŒ‘é€‰å‡ºçš„å‚è€ƒæ¥åšâ€œå‚è€ƒâ†’probeâ€çš„å åŠ 
    dtheta = gp.dtheta_manual
    scale  = gp.scale_manual
    R = np.array([[np.cos(dtheta), -np.sin(dtheta)],
                [np.sin(dtheta),  np.cos(dtheta)]], dtype=np.float64)
    ref0, probe0 = ref_sel[0], probe_xy[0]
    ref_xy_in_probe = (ref_sel - ref0) @ R.T * scale + probe0

    # --- ç»˜å›¾ ---
    fig, ax = plt.subplots(figsize=(8, 6), constrained_layout=False)

    def _every_k_indices(n, k):
        idxs = np.arange(0, n, k, dtype=int)
        if len(idxs) == 0 or idxs[-1] != n - 1:
            idxs = np.r_[idxs, n - 1]
        return idxs

    kstep = int(MAX_DATA_PER_EXPERT)

    # âœ… ç”¨è¢«é€‰ä¸­çš„å‚è€ƒ ref_selï¼Œè€Œä¸æ˜¯ ref_xy
    ref_idx   = _every_k_indices(ref_sel.shape[0],   kstep)
    probe_idx = _every_k_indices(probe_xy.shape[0], kstep)

    # --- ç”»å¯¹åº”çš„ä¸¤ä¸ªé”šå‘é‡ï¼ˆorigin -> anchorï¼‰ ---
    if getattr(gp, "anchor", None) is not None:
        # âœ… ref çš„åŸç‚¹ä¹Ÿè¦æ¥è‡ª ref_sel
        o_ref = ref_sel[0]
        p_ref = gp.anchor["ref_point"]
        o_pro = probe_xy[0]
        p_pro = gp.anchor["probe_point"]

        ax.plot([o_ref[0], p_ref[0]], [o_ref[1], p_ref[1]],
                '--', linewidth=2, color='C3',
                label=f"Ref anchor vec (idx={gp.anchor['ref_index']})", zorder=4)
        ax.plot([o_pro[0], p_pro[0]], [o_pro[1], p_pro[1]],
                '--', linewidth=2, color='C4',
                label=f"Probe anchor vec (idx={gp.anchor['probe_index']})", zorder=4)

        ax.scatter([o_ref[0], p_ref[0]], [o_ref[1], p_ref[1]],
                s=32, marker='o', color='C3', zorder=5)
        ax.scatter([o_pro[0], p_pro[0]], [o_pro[1], p_pro[1]],
                s=32, marker='x', color='C4', zorder=5)

    # âœ… è¿™é‡Œç”¨ ref_sel ç”»â€œè¢«é€‰ä¸­çš„å‚è€ƒâ€
    ax.plot(ref_sel[:, 0],  ref_sel[:, 1],  label="Reference (selected)",  zorder=3)
    ax.plot(probe_xy[:, 0], probe_xy[:, 1], label="Probe", zorder=3)
    if preds is not None and len(preds) > 0:
        ax.plot(preds[:, 0], preds[:, 1], label="Prediction", zorder=3)

    # å åŠ â€œå‚è€ƒâ†’Probeâ€çš„å¯¹é½æ›²çº¿ï¼ˆä½ å‰é¢å·²ç»ç”¨ ref_sel ç®—å¥½äº† ref_xy_in_probeï¼‰
    ax.plot(ref_xy_in_probe[:, 0], ref_xy_in_probe[:, 1], '--', linewidth=2,
            label=f"Referenceâ†’Probe (Î¸={np.degrees(gp.dtheta_manual):.1f}Â°, s={gp.scale_manual:.3f})",
            zorder=3)

    ax.set_aspect("equal", adjustable="box")
    ax.grid(True, alpha=0.3)
    ax.legend(loc="center left", bbox_to_anchor=(1.02, 0.5),
            borderaxespad=0., frameon=True, framealpha=0.85)
    plt.subplots_adjust(right=0.78)
    ax.set_title("Ref / Probe / Prediction (main)")
    ax.set_xlabel("x"); ax.set_ylabel("y")
    plt.tight_layout()
    plt.savefig("ref_probe_prediction_main.png", dpi=150, bbox_inches="tight")

    plot_anchor_vectors_from_gp(gp)
    plot_relative_angle_bases_from_gp(gp, k_hist=K_HIST, filename="relative_angle_bases.png")
    # Save predictions to CSV
    if preds is not None and len(preds) > 0:
        with open("preds.csv", "w", newline="") as f:
            writer = csv.writer(f)
            writer.writerow(["x_pred", "y_pred"])
            writer.writerows(preds.tolist())
        print(f"Saved predictions to preds.csv, shape={preds.shape}")